\documentclass[letterpaper,10pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{amsthm,amssymb,amsmath}
%% H/T: https://tex.stackexchange.com/a/202168/32270
\usepackage{textcomp}
%% H/T: https://tex.stackexchange.com/a/56877/32270
\usepackage{algorithm}
\usepackage{algpseudocode}
%% H/T: https://tex.stackexchange.com/a/244805
\usepackage{booktabs,siunitx}

\usepackage[usenames, dvipsnames]{color}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  urlcolor=blue,
  citecolor=ForestGreen,
  pdfinfo={
    CreationDate={D:20180425160342},
    ModDate={D:20180425160342},
  },
}

\usepackage{embedfile}
\embedfile{\jobname.tex}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\(K\)-compensated de Casteljau}
\rhead{Danny Hermes}

\renewcommand{\headrulewidth}{0pt}
\newcommand{\cond}[1]{\operatorname{cond}\left(#1\right)}
\newcommand{\fl}[1]{\operatorname{fl}\left(#1\right)}
\newcommand{\eps}{\varepsilon}

\begin{document}

\begin{abstract}
In computer aided geometric design a polynomial is usually represented in
Bernstein form. This paper presents a family of compensated algorithms to
accurately evaluate a polynomial in Bernstein form with floating point
coefficients. The principle is to apply error-free transformations to
improve the traditional de Casteljau algorithm. At each stage of computation,
round-off error is passed on to first order errors, then to second order
errors, and so on. After the computation has been ``filtered'' \(K\)-times
via this process, the resulting output is as accurate as the de Casteljau
algorithm performed in \(K\) times the working precision. Forward error
analysis and numerical experiments illustrate the accuracy of this family
of algorithms.
\end{abstract}

\tableofcontents

\section{IEEE Stuff}

\begin{algorithm}[H]
  \caption{\textit{EFT of the sum of two floating point numbers.}}

  \begin{algorithmic}
    \Function{\(\left[S, \sigma\right] = \mathtt{TwoSum}\)}{$a, b$}
      \State \(S = a \oplus b\)
      \State \(z = S \ominus a\)
      \State \(\sigma = (a \ominus (S \ominus z)) \oplus (b \ominus z)\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{Splitting of a floating point number into two parts.}}

  \begin{algorithmic}
    \Function{\(\left[h, \ell\right] = \mathtt{Split}\)}{$a$}
      \State \(z = a \otimes (2^r + 1)\)
      \State \(h = z \ominus (z \ominus a)\)
      \State \(\ell = a \ominus h\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{EFT of the product of two floating point numbers.}}

  \begin{algorithmic}
    \Function{\(\left[P, \pi\right] = \mathtt{TwoProd}\)}{$a, b$}
      \State \(P = a \otimes b\)
      \State \(\left[a_h, a_{\ell}\right] = \mathtt{Split}(a)\)
      \State \(\left[b_h, b_{\ell}\right] = \mathtt{Split}(b)\)
      \State \(\pi = a_{\ell} \otimes b_{\ell} \ominus (((P \ominus
          a_h \otimes b_h)
          \ominus a_{\ell} \otimes b_h) \ominus a_h \otimes b_{\ell})\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{EFT of the sum of two floating point numbers with a FMA.}}

  \begin{algorithmic}
    \Function{\(\left[P, \pi\right] = \mathtt{TwoProdFMA}\)}{$a, b$}
      \State \(P = a \otimes b\)
      \State \(\pi = \mathtt{FMA}(a, b, -P)\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{de Casteljau algorithm for polynomial evaluation.}}

  \begin{algorithmic}
    \Function{\(\mathtt{res} = \mathtt{DeCasteljau}\)}{$\mathbf{b}, s$}
      \State \(n = \texttt{length}(b) - 1\)
      \State \(r = 1 \ominus s\)
      \\
      \For{\(j = 0, \ldots, n\)}
        \State \(b_j^{(n)} = b_j\)
      \EndFor
      \\
      \For{\(k = n - 1, \ldots, 0\)}
        \For{\(j = 0, \ldots, k\)}
          \State \(b_j^{(k)} = \left(r \otimes b_j^{(k + 1)}\right) \oplus
              \left(s \otimes b_{j + 1}^{(k + 1)}\right)\)
        \EndFor
      \EndFor
      \\
      \State \(\mathtt{res} = b_0^{(0)}\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{Compensated de Casteljau algorithm for polynomial evaluation.}}

  \begin{algorithmic}
    \Function{\(\mathtt{res} = \mathtt{CompDeCasteljau}\)}{$\mathbf{b}, s$}
      \State \(n = \texttt{length}(\mathbf{b}) - 1\)
      \State \(\left[r, \rho\right] = \mathtt{TwoSum}(1, -s)\)
      \\
      \For{\(j = 0, \ldots, n\)}
        \State \(b_j^{(n)} = b_j\)
        \State \(\Delta b_j^{(n)} = 0\)
      \EndFor
      \\
      \For{\(k = n - 1, \ldots, 0\)}
        \For{\(j = 0, \ldots, k\)}
          \State \(\left[P_1, \pi_1\right] = \mathtt{TwoProd}\left(
              r, b_j^{(k + 1)}\right)\)
          \State \(\left[P_2, \pi_2\right] = \mathtt{TwoProd}\left(
              s, b_{j + 1}^{(k + 1)}\right)\)
          \State \(\left[b_j^{(k)}, \sigma_3\right] = \mathtt{TwoSum}(
              P_1, P_2)\)
          \State \(w = \pi_1 \oplus \pi_2 \oplus \sigma_3 \oplus
              \left(\rho \otimes b_j^{(k + 1)}\right)\)
          \State \(\Delta b_j^{(k)} = w \oplus \left(s \otimes
              \Delta b_{j + 1}^{(k + 1)}
              \right) \oplus \left(r \otimes \Delta b_j^{(k + 1)}\right)\)
        \EndFor
      \EndFor
      \\
      \State \(\mathtt{res} = b_0^{(0)} \oplus \Delta b_0^{(0)}\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{de Casteljau's Method}

Consider de Casteljau's method to evaluate a degree \(n\)
polynomial in Bernstein-B\'{e}zier form with control points \(b_j\):
\begin{align*}
    b_j^{(n)} &= b_j \\
    b_j^{(k)} &= (1 - s) b_j^{(k + 1)} + s b_{j + 1}^{(k + 1)} \\
    b(s) &= b_0^{(0)}.
\end{align*}

\subsection{Condition Number}

For a polynomial \(p(x)\) in the power basis, we have
(\cite{langlois_et_al:DSP:2006:442}):
\[\cond{p(x)} = \frac{\widetilde{p}\left(\left|x\right|\right)}{
  \left|p(x)\right|} = \frac{\sum_j \left|a_j\right| \left|x\right|^j}{
  \left|p(x)\right|}.\]
In particular, this means that if \(x \geq 0\) and each \(a_j \geq 0\)
we must necessarily have \(\cond{p(x)} = 1\). To see an example in use,
consider \(p(x) = (x - 1)^n\) and input values of the form \(x = 1 + \delta\)
(with \(\left|\delta\right| \ll 1\)). Since \(a_j = \binom{n}{j} (-1)^{n - j}\)
we have \(\widetilde{p}(x) = (x + 1)^n\) hence
\[\cond{p\left(1 + \delta\right)} = \frac{(2 + \delta)^n}{
  \left|\delta\right|^n} = \left|1 + \frac{2}{\delta}\right|^n.\]
As \(\delta \to 0\), this value approaches \(\infty\) (as expected).

For a polynomial \(p(s)\) in Bernstein form, we have (\cite{Jiang2010}):
\[\cond{p(s)} = \frac{\widetilde{p}\left(s\right)}{
  \left|p(s)\right|} = \frac{\sum_j \left|b_j\right| \left|B_{j, n}(s)\right|}{
  \left|p(s)\right|}.\]
The Bernstein form is suited for \(s \in \left[0, 1\right]\), which means
\(B_{j, n}(s) \geq 0\) typically. If \(s \in \left[0, 1\right]\) and each
\(b_j \geq 0\) we must necessarily have \(\cond{p(s)} = 1\). To see an
example in use, consider
\[p(s) = (1 - 2s)^n = \left[(1 - s) - s\right]^n = \sum_j \binom{n}{j}
(1 - s)^{n - j} (-s)^j = \sum_j (-1)^j B_{j, n}(s)\]
and input values of the form \(x = \frac{1}{2} + \delta\)
(with \(\left|\delta\right| \ll \frac{1}{2}\)). Since \(b_j = (-1)^j\)
we have \(\widetilde{p}(s) = \left[(1 - s) + s\right]^n = 1\)
\[\cond{p\left(\frac{1}{2} + \delta\right)} = \frac{1}{
  \left|2\delta\right|^n}.\]
As \(\delta \to 0\), this value approaches \(\infty\) (as expected).

\subsection{Example of Compensated de Casteljau Failing}

Consider
\[p(s) = (4s - 3)^3 (8s + 7) = (-189) B_{0, 4}(s) + (-54) B_{1, 4}(s) +
57 B_{2, 4}(s) + (-32) B_{3, 4}(s) + 15 B_{4, 4}(s)\]
at the point \(s = \frac{3}{4} + 800 u\). In exact arithmetic, we
have \(p(s) = 2^{38.6} u^3 + \mathcal{O}\left(u^4\right)\).
However, using the compensated de Casteljau algorithm results in
\(b_0^{(0)} + \Delta b_0^{(0)} = 0\):

\begin{center}
  \begin{tabular}{>{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$}}
    \toprule
    k & j & b_j^{(k)} & \Delta b_j^{(k)} & \Delta^2 b_j^{(k)} & \Delta^3 b_j^{(k)} \\
    \midrule
    3 & 0 & -87\frac{3}{4} + 108032u & -32u & 0 & 0 \\
    3 & 1 & 29\frac{1}{4} + 88768u & 32u & 0 & 0 \\
    3 & 2 & -9\frac{3}{4} - 71200u & 0 & 0 & 0 \\
    3 & 3 & 3\frac{1}{4} + 37600u & 0 & 0 & 0 \\
    \midrule
    2 & 0 & 187200u & -15360000u^2 & 0 & 0 \\
    2 & 1 & -62408u & 8u - 128000000u^2 & 0 & 0 \\
    2 & 2 & 20800u & 87040000u^2 & 0 & 0 \\
    \midrule
    1 & 0 & -6u - 199688192u^2 & 6u - 99831808u^2 & -90112000000u^3 & 0 \\
    1 & 1 & -2u + 66568192u^2  & 2u + 33271808u^2 & 172032000000u^3 & 0 \\
    \midrule
    0 & 0 & -3u + 7296u^2 & 3u - 7296u^2 & 13 (3200u)^3 + 3048(512u)^4 & 962(128u)^4 \\
    \bottomrule
  \end{tabular}
\end{center}

\subsection{Selection of Test Cases}

From \cite{Delgado2015} (end of Section 3):

\begin{quote}
  We can observe that, in this case, the algorithm with a good
  behavior everywhere is the de Casteljau algorithm
\end{quote}

\noindent In the same paper (when referring to \cite{Bezerra2013} at the
beginning of Section 2):

\begin{quote}
  assuming that all control points are positive. This assumption avoided
  ill-conditioned polynomials. In this section, we shall show that this is
  a natural assumption in Computer Aided Geometric Design (from now on,
  C.A.G.D.) and that it permits to assure high relative precision for the
  evaluation through a large family of representations in C.A.G.D.
\end{quote}

\noindent From the same author, in \cite{Mainar2005} (towards the
end of Section 5, at the bottom of page 109):

\begin{quote}
  Let us observe that in this case, the de Casteljau algorithm presents
  better stability properties for the evaluation near the roots. In fact,
  the de Casteljau algorithm has good behaviour even when using simple
  precision, although the running error bound is not so accurate in points
  close to the roots.
\end{quote}

\subsection{\texorpdfstring{\(K\)}{K}-Fold Error Filtering}

After implementing for \(K = 2, 3, \ldots, 12\) and instrumenting all
relevant floating point operations, the \(K\)-fold Horner requires
\[(5 \cdot 2^K - 8)n + \left((K + 8) 2^K - 12K - 6\right) =
\mathcal{O}\left((n + K)2^K\right)\]
flops to evaluate a degree \(n\) polynomial (this only applies when
\(n \geq K - 1\)). As a comparison, the
non-compensated form of Horner requires \(2n\) flops. Of these,
\(\left(2^{K - 1} - 1\right)n - 2^{K - 1}(K - 3) - 2\) are
FMA (fused-multiply-add) instructions.

After implementing for \(K = 2, 3, 4, 5\) and instrumenting all relevant
floating point operations, the \(K\)-fold de Casteljau requires
\[(15K^2 - 34K + 26)T_n + K + 5 =
\mathcal{O}\left(n^2 K^2\right)\]
flops to evaluate a degree \(n\) polynomial. (Here \(T_n\) is the
\(n\)th triangular number.) As a comparison, the non-compensated form of
de Casteljau requires \(3 T_n + 1\) flops. Of these, \((3K - 4)T_n\) are
FMA instructions. On hardware that doesn't support FMA,
every FMA will be exchanged for 10 \(\ominus\)'s and 6 \(\otimes\)'s so the
count will increase by \((10 + 6 - 1)(3K - 4)T_n\).

\section{Bogus Section for Refs}

Here they are, for now
\begin{itemize}
  \item Compensated Horner (\(K = 2\)) (\cite{langlois_et_al:DSP:2006:442})
  \item Compensated de Casteljau (\cite{Jiang2010})
  \item Newton with compensated Horner (\cite{Graillat2008})
  \item \(K\)-fold Sum (\cite{Ogita2005})
  \item \(K\)-fold Horner (\cite{Graillat2009})
\end{itemize}

\bibliography{paper}
\bibliographystyle{alpha}

\end{document}
