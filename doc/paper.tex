\documentclass[letterpaper,10pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{amsthm,amssymb,amsmath}
%% H/T: https://tex.stackexchange.com/a/202168/32270
\usepackage{textcomp}
%% H/T: https://tex.stackexchange.com/a/56877/32270
\usepackage{algorithm}
\usepackage{algpseudocode}
%% H/T: https://tex.stackexchange.com/a/244805
\usepackage{booktabs,siunitx}
%% H/T: https://tex.stackexchange.com/a/4217/32270
\usepackage{mathtools}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}

\usepackage[usenames, dvipsnames]{color}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  urlcolor=blue,
  citecolor=ForestGreen,
  pdfinfo={
    CreationDate={D:20180425160342},
    ModDate={D:20180425160342},
  },
}

%% H/T: https://tex.stackexchange.com/a/99051/32270
\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{box} = [
  rectangle, minimum width=0.1\textwidth, minimum height=0.1\textwidth,
  text centered, draw=black]
\tikzstyle{empty-box} = [
  rectangle, minimum width=0.1\textwidth, minimum height=0.1\textwidth,
  text width=0.05\textwidth]
\tikzstyle{empty-box-vert} = [
  rectangle, minimum width=0.1\textwidth, minimum height=0.1\textwidth,
  text centered, text height=0.001\textwidth]
\tikzstyle{line} = [draw, -latex']

\usepackage{embedfile}
\embedfile{\jobname.tex}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\(K\)-compensated de Casteljau}
\rhead{Danny Hermes}

\renewcommand{\headrulewidth}{0pt}
\newcommand{\cond}[1]{\operatorname{cond}\left(#1\right)}
\newcommand{\fl}[1]{\operatorname{fl}\left(#1\right)}
\newcommand{\eps}{\varepsilon}
\newcommand{\mach}{\mathbf{u}}

\begin{document}

\begin{abstract}
In computer aided geometric design a polynomial is usually represented in
Bernstein form. This paper presents a family of compensated algorithms to
accurately evaluate a polynomial in Bernstein form with floating point
coefficients. The principle is to apply error-free transformations to
improve the traditional de Casteljau algorithm. At each stage of computation,
round-off error is passed on to first order errors, then to second order
errors, and so on. After the computation has been ``filtered'' \((K - 1)\)
times via this process, the resulting output is as accurate as the de Casteljau
algorithm performed in \(K\) times the working precision. Forward error
analysis and numerical experiments illustrate the accuracy of this family
of algorithms.
\end{abstract}

\tableofcontents

\section{Introduction}

In computer aided geometric design, polynomials are usually expressed in
Bernstein form. Polynomials in this form are usually evaluated by the
de Casteljau algorithm. This algorithm has a round-off error bound
which grows only linearly with degree, even though the number of
arithmetic operations grows quadratically. Nevertheless the de Casteljau
algorithm returns results arbitrarily less accurate than the working
precision \(\mach\) when evaluating \(p(s)\) is ill-conditioned.
The relative accuracy of the computed
evaluation with the de Casteljau algorithm (\texttt{DeCasteljau}) satisfies
(\cite{Mainar1999}) the following a priori bound:
\begin{equation}\label{de-casteljau-error}
  \frac{\left|p(s) - \mathtt{DeCasteljau}(p, s)\right|}{\left|p(s)\right|} \leq
  \cond{p, s} \times \mathcal{O}(\mach).
\end{equation}
In the right-hand side of this inequality, \(\mach\) is the computing
precision and the condition number \(\cond{p, s} \geq 1\) only depends
on \(s\) and the Bernstein coefficients of \(p\) --- its expression will
be given further.

For ill-conditioned problems, such as evaluating \(p(s)\) near a
multiple root, the condition number may be arbitrarily large, i.e.
\(\cond{p, s} > 1 / \mach\), in
which case most or all of the computed digits will be incorrect.
In some cases, even the order of magnitude of the computed value
of \(p(s)\) can be incorrect.

To address ill-conditioned problems, error-free transformations (EFT) can
be applied in \textit{compensated algorithms} to account for roundoff.
Error-free transformations were studied in great detail in \cite{Ogita2005}
and open a large number of applications.
In \cite{langlois_et_al:DSP:2006:442}, a compensated Horner's algorithm was
described to evaluate a polynomial in the monomial basis. In \cite{Jiang2010},
a similar method was described to perform a compensated version of the de
Casteljau algorithm. In both cases, the \(\cond{p, s}\) factor is moved
from \(\mach\) to \(\mach^2\) and the computed value is as accurate
as if the computations were done in twice the working precision. For example,
the compensated de Casteljau algorithm (\texttt{CompDeCasteljau}) satisfies
\begin{equation}\label{de-casteljau-2-error}
  \frac{\left|p(s) - \mathtt{CompDeCasteljau}(p, s)\right|}{
    \left|p(s)\right|} \leq \mach + \cond{p, s} \times
    \mathcal{O}\left(\mach^2\right).
\end{equation}
For problems with \(\cond{p, s} < 1 / \mach^2\), the relative error
is \(\mach\), i.e. accurate to full precision, aside from rounding to the
nearest floating point number.

In \cite{Graillat2009}, the authors built on the compensated Horner's
algorithm to produce a method for evaluating a polynomial as if
the computations were done in \(K\) times the working precision for
any \(K \geq 2\). This result motivates this paper, though the
approach there is somewhat different than ours. They perform each computation
with error-free transformations and interpret the errors as coefficients of new
polynomials. They then evaluate the error polynomials, which (recursively)
generate second order error polynomials and so on. This recursive property
causes the number of operations to grow exponentially in \(K\). Here, we
instead have a fixed number of error groups, each corresponding to roundoff
from the group above it. For example, when
\((1 - s) b_j^{(n)} + s b_{j + 1}^{(n)}\) is computed in floating point, any
error is filtered down to the error group below it.

As in~\eqref{de-casteljau-error}, the accuracy of the compensated
result~\eqref{de-casteljau-2-error} may be arbitrarily bad for ill-conditioned
polynomial evaluations. We describe how to defer rounding into progressively
smaller error groups and improve the accuracy of the computed result by a
factor of \(\mach\) for every error group added. So we derive
\texttt{CompDeCasteljauK}, a \(K\)-fold compensated de Casteljau algorithm
that satisfies the following a prior bound for any arbitrary integer \(K\):
\begin{equation}
  \frac{\left|p(s) - \mathtt{CompDeCasteljauK}(p, s)\right|}{
    \left|p(s)\right|} \leq \mach + \cond{p, s} \times
    \mathcal{O}\left(\mach^K\right).
\end{equation}
This means that the computed value with \texttt{CompDeCasteljauK} is now
as accurate as the result of the de Casteljau algorithm performed in
\(K\) times the working precision with a final rounding back to this
working precision.

The paper is organized as follows. Section~\ref{notation} introduces some
basic notation and results about floating point arithmetic, error-free
transformations and the de Casteljau algorithm. In Section~\ref{compensated-2},
the compensated algorithm for polynomial evaluation from \cite{Jiang2010} is
reviewed and notation is established for the expansion. In
Section~\ref{compensated-k}, the \(K\) compensated algorithm is provided and
a forward error analysis is performed. Finally, in Section~\ref{numerical} we
give numerical tests to illustrate the practical efficiency of our algorithms.

\section{Basic notation and results}\label{notation}

Throughout this paper we assume that the computation in floating point
arithmetic obeys the model
\begin{equation}
  a \star b = \fl{a \circ b} = (a \circ b)(1 + \eps_1) =
  (a \circ b) / (1 + \eps_2)
\end{equation}
where \(\star \in \left\{\oplus, \ominus, \otimes, \oslash\right\}\), \(\circ
\in \left\{+, -, \times, \div\right\}\) and \(\left|\eps_1\right|,
\left|\eps_2\right| \leq \mach\). The symbol \(\mach\) is the unit round-off
and \(\star\) represents the floating point computation, e.g.
\(a \oplus b = \fl{a + b}\). We also assume that the computed result of
\(\alpha \in \mathbf{R}\) in floating point arithmetic is denoted by
\(\widehat{\alpha}\) or \(\fl{\alpha}\) and \(\mathbf{F}\) denotes the set of
all floating point numbers (see \cite{Higham2002} for more details).
Following \cite{Higham2002}, we will use the following classic properties in
error analysis.

\begin{enumerate}
  \item If \(\delta_i \leq \mach\), \(\rho_i = \pm 1\), then
      \(\prod_{i = 1}^n (1 + \delta_i)^{\rho_i} = 1 + \theta_n\),
  \item \(\left|\theta_n\right| \leq \gamma_n \coloneqq
      n \mach / (1 - n \mach)\),
  \item \((1 + \theta_k)(1 + \theta_j) \leq (1 + \theta_{k + j})\),
  \item \(\gamma_k + \gamma_j + \gamma_k \gamma_j \leq \gamma_{k + j}\),
  \item \((1 + \mach)^j \leq 1 / (1 - j \mach)\).
\end{enumerate}

Now let us introduce some results concerning error-free transformation (EFT).
For a pair floating point numbers \(a, b \in \mathbf{F}\), there exists
a floating point number \(y\) satisfying \(a \circ b = x + y\) where
\(x = \fl{a \circ b}\) and \(\circ \in \left\{+, -, \times\right\}\).
The transformation \((a, b) \longrightarrow (x, y)\) is regarded as an
error-free transformation. The error-free transformation algorithms of the
sum and product of two floating point numbers used later in this paper are
the \texttt{TwoSum} algorithm by Knuth \cite{Knuth1997} and \texttt{TwoProd}
algorithm by Dekker \cite{Dekker1971}, respectively. The following theorem
exhibits the important properties of \texttt{TwoSum} and \texttt{TwoProd}
algorithms (see \cite{Ogita2005}).

\begin{theorem}[\cite{Ogita2005}]
For \(a, b \in \mathbf{F}\) and \(x, y \in \mathbf{F}\), \texttt{TwoSum} and
\texttt{TwoProd} satisfy
\begin{itemize}
  \item \(\left[x, y\right] = \mathtt{TwoSum}(a, b)\), \(x = \fl{a + b}\),
      \(x + y = a + b\), \(y \leq \mach \left|x\right|\),
      \(y \leq \mach \left|a + b\right|\)
  \item \(\left[x, y\right] = \mathtt{TwoProd}(a, b)\),
      \(x = \fl{a \times b}\), \(x + y = a \times b\),
      \(y \leq \mach \left|x\right|\),
      \(y \leq \mach \left|a \times b\right|\).
\end{itemize}
\end{theorem}

Next, we recall the de Casteljau algorithm:

\begin{algorithm}[H]
  \caption{\textit{de Casteljau algorithm for polynomial evaluation.}}

  \begin{algorithmic}
    \Function{\(\mathtt{res} = \mathtt{DeCasteljau}\)}{$\mathbf{b}, s$}
      \State \(n = \texttt{length}(b) - 1\)
      \State \(r = 1 \ominus s\)
      \\
      \For{\(j = 0, \ldots, n\)}
        \State \(b_j^{(n)} = b_j\)
      \EndFor
      \\
      \For{\(k = n - 1, \ldots, 0\)}
        \For{\(j = 0, \ldots, k\)}
          \State \(b_j^{(k)} = \left(r \otimes b_j^{(k + 1)}\right) \oplus
              \left(s \otimes b_{j + 1}^{(k + 1)}\right)\)
        \EndFor
      \EndFor
      \\
      \State \(\mathtt{res} = b_0^{(0)}\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{theorem}[\cite{Mainar1999}]
If \(p(s) = \sum_{j = 0}^n b_j B_{j, n}(s)\) and \(\mathtt{DeCasteljau}(p, s)\)
is the value computed by the de Casteljau algorithm then
\begin{equation}
\left|p(s) - \mathtt{DeCasteljau}(p, s)\right| \leq \gamma_{2n}
\sum_{j = 0}^n \left|b_j\right| B_{j, n}(s).
\end{equation}
\end{theorem}

The relative condition number of the evaluation of \(p(s) = \sum_{j = 0}^n
b_j B_{j, n}(s)\) in Bernstein form used in this paper is (see
\cite{Mainar1999}, \cite{Farouki1987}):
\begin{equation}
\cond{p, s} = \frac{\widetilde{p}\left(s\right)}{
  \left|p(s)\right|} = \frac{\sum_j \left|b_j\right| B_{j, n}(s)}{
  \left|p(s)\right|},
\end{equation}
where \(B_{j, n} \geq 0\).

\section{Compensated de Casteljau}\label{compensated-2}

Placeholder

\section{\texorpdfstring{\(K\)}{K} Compensated de Casteljau}\label{compensated-k}

Placeholder

\section{Numerical experiments}\label{numerical}

Placeholder

\section{IEEE Stuff}

\begin{algorithm}[H]
  \caption{\textit{EFT of the sum of two floating point numbers.}}

  \begin{algorithmic}
    \Function{\(\left[S, \sigma\right] = \mathtt{TwoSum}\)}{$a, b$}
      \State \(S = a \oplus b\)
      \State \(z = S \ominus a\)
      \State \(\sigma = (a \ominus (S \ominus z)) \oplus (b \ominus z)\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{Splitting of a floating point number into two parts.}}

  \begin{algorithmic}
    \Function{\(\left[h, \ell\right] = \mathtt{Split}\)}{$a$}
      \State \(z = a \otimes (2^r + 1)\)
      \State \(h = z \ominus (z \ominus a)\)
      \State \(\ell = a \ominus h\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{EFT of the product of two floating point numbers.}}

  \begin{algorithmic}
    \Function{\(\left[P, \pi\right] = \mathtt{TwoProd}\)}{$a, b$}
      \State \(P = a \otimes b\)
      \State \(\left[a_h, a_{\ell}\right] = \mathtt{Split}(a)\)
      \State \(\left[b_h, b_{\ell}\right] = \mathtt{Split}(b)\)
      \State \(\pi = a_{\ell} \otimes b_{\ell} \ominus (((P \ominus
          a_h \otimes b_h)
          \ominus a_{\ell} \otimes b_h) \ominus a_h \otimes b_{\ell})\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{EFT of the sum of two floating point numbers with a FMA.}}

  \begin{algorithmic}
    \Function{\(\left[P, \pi\right] = \mathtt{TwoProdFMA}\)}{$a, b$}
      \State \(P = a \otimes b\)
      \State \(\pi = \mathtt{FMA}(a, b, -P)\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{\textit{Compensated de Casteljau algorithm for polynomial evaluation.}}

  \begin{algorithmic}
    \Function{\(\mathtt{res} = \mathtt{CompDeCasteljau}\)}{$\mathbf{b}, s$}
      \State \(n = \texttt{length}(\mathbf{b}) - 1\)
      \State \(\left[r, \rho\right] = \mathtt{TwoSum}(1, -s)\)
      \\
      \For{\(j = 0, \ldots, n\)}
        \State \(b_j^{(n)} = b_j\)
        \State \(\partial b_j^{(n)} = 0\)
      \EndFor
      \\
      \For{\(k = n - 1, \ldots, 0\)}
        \For{\(j = 0, \ldots, k\)}
          \State \(\left[P_1, \pi_1\right] = \mathtt{TwoProd}\left(
              r, b_j^{(k + 1)}\right)\)
          \State \(\left[P_2, \pi_2\right] = \mathtt{TwoProd}\left(
              s, b_{j + 1}^{(k + 1)}\right)\)
          \State \(\left[b_j^{(k)}, \sigma_3\right] = \mathtt{TwoSum}(
              P_1, P_2)\)
          \State \(w = \pi_1 \oplus \pi_2 \oplus \sigma_3 \oplus
              \left(\rho \otimes b_j^{(k + 1)}\right)\)
          \State \(\partial b_j^{(k)} = w \oplus \left(s \otimes
              \partial b_{j + 1}^{(k + 1)}
              \right) \oplus \left(r \otimes \partial b_j^{(k + 1)}\right)\)
        \EndFor
      \EndFor
      \\
      \State \(\mathtt{res} = b_0^{(0)} \oplus \partial b_0^{(0)}\)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{de Casteljau's Method}

Consider de Casteljau's method to evaluate a degree \(n\)
polynomial in Bernstein-B\'{e}zier form with control points \(b_j\):
\begin{align*}
    b_j^{(n)} &= b_j \\
    b_j^{(k)} &= (1 - s) b_j^{(k + 1)} + s b_{j + 1}^{(k + 1)} \\
    b(s) &= b_0^{(0)}.
\end{align*}

\subsection{Condition Number}

For a polynomial \(p(x)\) in the power basis, we have
(\cite{langlois_et_al:DSP:2006:442}):
\begin{equation}
\cond{p(x)} = \frac{\widetilde{p}\left(\left|x\right|\right)}{
  \left|p(x)\right|} = \frac{\sum_j \left|a_j\right| \left|x\right|^j}{
  \left|p(x)\right|}.
\end{equation}
In particular, this means that if \(x \geq 0\) and each \(a_j \geq 0\)
we must necessarily have \(\cond{p(x)} = 1\). To see an example in use,
consider \(p(x) = (x - 1)^n\) and input values of the form \(x = 1 + \delta\)
(with \(\left|\delta\right| \ll 1\)). Since \(a_j = \binom{n}{j} (-1)^{n - j}\)
we have \(\widetilde{p}(x) = (x + 1)^n\) hence
\begin{equation}
\cond{p\left(1 + \delta\right)} = \frac{(2 + \delta)^n}{
  \left|\delta\right|^n} = \left|1 + \frac{2}{\delta}\right|^n.
\end{equation}
As \(\delta \to 0\), this value approaches \(\infty\) (as expected).

For a polynomial \(p(s)\) in Bernstein form, we have (\cite{Jiang2010}):
\begin{equation}
\cond{p(s)} = \frac{\widetilde{p}\left(s\right)}{
  \left|p(s)\right|} = \frac{\sum_j \left|b_j\right| \left|B_{j, n}(s)\right|}{
  \left|p(s)\right|}.
\end{equation}
The Bernstein form is suited for \(s \in \left[0, 1\right]\), which means
\(B_{j, n}(s) \geq 0\) typically. If \(s \in \left[0, 1\right]\) and each
\(b_j \geq 0\) we must necessarily have \(\cond{p(s)} = 1\). To see an
example in use, consider
\begin{equation}
p(s) = (1 - 2s)^n = \left[(1 - s) - s\right]^n = \sum_j \binom{n}{j}
(1 - s)^{n - j} (-s)^j = \sum_j (-1)^j B_{j, n}(s)
\end{equation}
and input values of the form \(x = \frac{1}{2} + \delta\)
(with \(\left|\delta\right| \ll \frac{1}{2}\)). Since \(b_j = (-1)^j\)
we have \(\widetilde{p}(s) = \left[(1 - s) + s\right]^n = 1\)
\begin{equation}
\cond{p\left(\frac{1}{2} + \delta\right)} = \frac{1}{
  \left|2\delta\right|^n}.
\end{equation}
As \(\delta \to 0\), this value approaches \(\infty\) (as expected).

\subsection{Example of Compensated de Casteljau Failing}

Consider
\begin{equation}
p(s) = (4s - 3)^3 (8s + 7) = (-189) B_{0, 4}(s) + (-54) B_{1, 4}(s) +
57 B_{2, 4}(s) + (-32) B_{3, 4}(s) + 15 B_{4, 4}(s)
\end{equation}
at the point \(s = \frac{3}{4} + 800 u\). In exact arithmetic, we
have \(p(s) = 13(3200u)^3 + \mathcal{O}\left(u^4\right)\).
However, using the compensated de Casteljau algorithm results in
\(b_0^{(0)} + \partial b_0^{(0)} = 0\):

\begin{center}
  \begin{tabular}{>{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$} >{$}c<{$}}
    \toprule
    k & j & b_j^{(k)} & \partial b_j^{(k)} & \partial^2 b_j^{(k)} & \partial^3 b_j^{(k)} \\
    \midrule
    3 & 0 & -87\frac{3}{4} + 108032u & -32u & 0 & 0 \\
    3 & 1 & 29\frac{1}{4} + 88768u & 32u & 0 & 0 \\
    3 & 2 & -9\frac{3}{4} - 71200u & 0 & 0 & 0 \\
    3 & 3 & 3\frac{1}{4} + 37600u & 0 & 0 & 0 \\
    \midrule
    2 & 0 & 187200u & -15360000u^2 & 0 & 0 \\
    2 & 1 & -62408u & 8u - 128000000u^2 & 0 & 0 \\
    2 & 2 & 20800u & 87040000u^2 & 0 & 0 \\
    \midrule
    1 & 0 & -6u - 199688192u^2 & 6u - 99831808u^2 & -90112000000u^3 & 0 \\
    1 & 1 & -2u + 66568192u^2  & 2u + 33271808u^2 & 172032000000u^3 & 0 \\
    \midrule
    0 & 0 & -3u + 7296u^2 & 3u - 7296u^2 & 13 (3200u)^3 + 3048(512u)^4 & 962(128u)^4 \\
    \bottomrule
  \end{tabular}
\end{center}

\subsection{Selection of Test Cases}

From \cite{Delgado2015} (end of Section 3):

\begin{quote}
  We can observe that, in this case, the algorithm with a good
  behavior everywhere is the de Casteljau algorithm
\end{quote}

\noindent In the same paper (when referring to \cite{Bezerra2013} at the
beginning of Section 2):

\begin{quote}
  assuming that all control points are positive. This assumption avoided
  ill-conditioned polynomials. In this section, we shall show that this is
  a natural assumption in Computer Aided Geometric Design (from now on,
  C.A.G.D.) and that it permits to assure high relative precision for the
  evaluation through a large family of representations in C.A.G.D.
\end{quote}

\noindent From the same author, in \cite{Mainar2005} (towards the
end of Section 5, at the bottom of page 109):

\begin{quote}
  Let us observe that in this case, the de Casteljau algorithm presents
  better stability properties for the evaluation near the roots. In fact,
  the de Casteljau algorithm has good behaviour even when using simple
  precision, although the running error bound is not so accurate in points
  close to the roots.
\end{quote}

\subsection{\texorpdfstring{\(K\)}{K} Error Filtering}

\begin{center}
\begin{tikzpicture}[node distance = 0.2\textwidth]
    \node [box] (bj) {\(b_j\)};
    \node [box, below of=bj] (bn0) {\(b_j^{(n)}\)};
    \node [box, right of=bn0] (dbn0) {\(\partial b_j^{(n)}\)};
    \node [box, right of=dbn0] (ddbn0) {\(\partial^2 b_j^{(n)}\)};
    \node [box, right of=ddbn0] (dddbn0) {\(\partial^3 b_j^{(n)}\)};
    %%
    \node [box, below of=bn0] (bn1) {\(b_j^{(n - 1)}\)};
    \node [box, below of=dbn0] (dbn1) {\(\partial b_j^{(n - 1)}\)};
    \node [box, below of=ddbn0] (ddbn1) {\(\partial^2 b_j^{(n - 1)}\)};
    \node [box, below of=dddbn0] (dddbn1) {\(\partial^3 b_j^{(n - 1)}\)};
    %%
    \node [box, below of=bn1] (bn2) {\(b_j^{(n - 2)}\)};
    \node [box, below of=dbn1] (dbn2) {\(\partial b_j^{(n - 2)}\)};
    \node [box, below of=ddbn1] (ddbn2) {\(\partial^2 b_j^{(n - 2)}\)};
    \node [box, below of=dddbn1] (dddbn2) {\(\partial^3 b_j^{(n - 2)}\)};
    %%
    \path [line] (bn0) -- node[above] {\(w_1^{(n)}\)} (dbn0);
    \path [line] (dbn0) -- node[above] {\(w_2^{(n)}\)} (ddbn0);
    \path [line] (ddbn0) -- node[above] {\(w_3^{(n)}\)} (dddbn0);
    \node [empty-box, right of=dddbn0] (right0) {\(\cdots\)};
    \path [line] (dddbn0) -- node[above] {\(w_4^{(n)}\)} (right0);
    %%
    \path [line] (bn1) -- node[above] {\(w_1^{(n - 1)}\)} (dbn1);
    \path [line] (dbn1) -- node[above] {\(w_2^{(n - 1)}\)} (ddbn1);
    \path [line] (ddbn1) -- node[above] {\(w_3^{(n - 1)}\)} (dddbn1);
    \node [empty-box, right of=dddbn1] (right1) {\(\cdots\)};
    \path [line] (dddbn1) -- node[above] {\(w_4^{(n - 1)}\)} (right1);
    %%
    \path [line] (bn2) -- node[above] {\(w_1^{(n - 2)}\)} (dbn2);
    \path [line] (dbn2) -- node[above] {\(w_2^{(n - 2)}\)} (ddbn2);
    \path [line] (ddbn2) -- node[above] {\(w_3^{(n - 2)}\)} (dddbn2);
    \node [empty-box, right of=dddbn2] (right2) {\(\cdots\)};
    \path [line] (dddbn2) -- node[above] {\(w_4^{(n - 2)}\)} (right2);
    %%
    \path [line] (bj) -- (bn0);
    \path [line] (bn0) -- node[fill=white] {\(U_n(s)\)} (bn1);
    \path [line] (bn1) -- node[fill=white] {\(U_{n - 1}(s)\)} (bn2);
    \node [empty-box-vert, below of=bn2] (bottom0) {\(\vdots\)};
    \path [line] (bn2) -- node[fill=white] {\(U_{n - 2}(s)\)} (bottom0);
    %%
    \path [line] (dbn0) -- node[fill=white] {\(U_n(s)\)} (dbn1);
    \path [line] (dbn1) -- node[fill=white] {\(U_{n - 1}(s)\)} (dbn2);
    \node [empty-box-vert, below of=dbn2] (bottom1) {\(\vdots\)};
    \path [line] (dbn2) -- node[fill=white] {\(U_{n - 2}(s)\)} (bottom1);
    %%
    \path [line] (ddbn0) -- node[fill=white] {\(U_n(s)\)} (ddbn1);
    \path [line] (ddbn1) -- node[fill=white] {\(U_{n - 1}(s)\)} (ddbn2);
    \node [empty-box-vert, below of=ddbn2] (bottom2) {\(\vdots\)};
    \path [line] (ddbn2) -- node[fill=white] {\(U_{n - 2}(s)\)} (bottom2);
    %%
    \path [line] (dddbn0) -- node[fill=white] {\(U_n(s)\)} (dddbn1);
    \path [line] (dddbn1) -- node[fill=white] {\(U_{n - 1}(s)\)} (dddbn2);
    \node [empty-box-vert, below of=dddbn2] (bottom3) {\(\vdots\)};
    \path [line] (dddbn2) -- node[fill=white] {\(U_{n - 2}(s)\)} (bottom3);
\end{tikzpicture}
\end{center}

\subsection{Operation Counts}

After implementing for \(K = 2, 3, \ldots, 12\) and instrumenting all
relevant floating point operations, the \(K\)-fold Horner requires
\begin{equation}
(5 \cdot 2^K - 8)n + \left((K + 8) 2^K - 12K - 6\right) =
\mathcal{O}\left((n + K)2^K\right)
\end{equation}
flops to evaluate a degree \(n\) polynomial (this only applies when
\(n \geq K - 1\)). As a comparison, the
non-compensated form of Horner requires \(2n\) flops. Of these,
\(\left(2^{K - 1} - 1\right)n - 2^{K - 1}(K - 3) - 2\) are
FMA (fused-multiply-add) instructions.

After implementing for \(K = 2, 3, 4, 5\) and instrumenting all relevant
floating point operations, the \(K\)-fold de Casteljau requires
\begin{equation}
(15K^2 - 34K + 26)T_n + K + 5 =
\mathcal{O}\left(n^2 K^2\right)
\end{equation}
flops to evaluate a degree \(n\) polynomial. (Here \(T_n\) is the
\(n\)th triangular number.) As a comparison, the non-compensated form of
de Casteljau requires \(3 T_n + 1\) flops. Of these, \((3K - 4)T_n\) are
FMA instructions. On hardware that doesn't support FMA,
every FMA will be exchanged for 10 \(\ominus\)'s and 6 \(\otimes\)'s so the
count will increase by \((10 + 6 - 1)(3K - 4)T_n\).

\section{Bogus Section for Refs}

Here they are, for now
\begin{itemize}
  \item Compensated Horner (\(K = 2\)) (\cite{langlois_et_al:DSP:2006:442})
  \item Compensated de Casteljau (\cite{Jiang2010})
  \item Newton with compensated Horner (\cite{Graillat2008})
  \item \(K\)-fold Sum (\cite{Ogita2005})
  \item \(K\)-fold Horner (\cite{Graillat2009})
\end{itemize}

\bibliography{paper}
\bibliographystyle{alpha}

\end{document}
